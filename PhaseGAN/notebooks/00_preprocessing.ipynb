{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import random\n",
    "\n",
    "from PhaseGAN.hyperparameter import hp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvianNatureSounds(Dataset):\n",
    "    def __init__(self,\n",
    "                # file args\n",
    "                annotation_file_path=None,\n",
    "                root_dir='../',\n",
    "                key='habitat',\n",
    "\n",
    "                # sound transformation args\n",
    "                mode='stft',\n",
    "                length=5,\n",
    "                sampling_rate=44100,\n",
    "                n_fft=1024,\n",
    "                hop_length=512,\n",
    "                downsample=True,\n",
    "                mel_spectrogram = None,\n",
    "                verbose=False,\n",
    "                fixed_limit=False\n",
    "                ):\n",
    "        \n",
    "        self.column = key\n",
    "        self.annotation_file = pd.read_csv(annotation_file_path).sort_values(self.column)\n",
    "        self.root_dir = root_dir\n",
    "        self.mel_transformation = mel_spectrogram\n",
    "        self.AmplitudeToDB = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.mode = mode\n",
    "        self.length = length\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.downsample = downsample\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.verbose = verbose\n",
    "        self.fixed_limit = fixed_limit\n",
    "        self.signal_length = None\n",
    "        self.griffin_lim = torchaudio.transforms.GriffinLim(n_fft=self.n_fft, \n",
    "                                                            win_length=self.n_fft,\n",
    "                                                            hop_length=self.hop_length,\n",
    "                                                            power=2,\n",
    "                                                            n_iter=5,\n",
    "                                                            momentum=0.99)\n",
    "        self.return_signal_dims()\n",
    "\n",
    "    def return_signal_dims(self):\n",
    "        if self.verbose:\n",
    "            print(f'Returning signal dimensions: H = {self.n_fft // 2 + 1} W = {((self.sampling_rate * self.length)//self.hop_length) + 1}')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_file)\n",
    "    \n",
    "    def _update_signal_length(self,signal_length):\n",
    "        self.signal_length = signal_length\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ####################################################################################\n",
    "        # Get stft spectrograms ############################################################\n",
    "        ####################################################################################\n",
    "        if self.mode == 'stft':\n",
    "            audio_sample_path = os.path.join(self.root_dir,os.listdir(self.root_dir)[index])\n",
    "            label = self.annotation_file.iloc[index][self.column]\n",
    "            signal, sr = torchaudio.load(audio_sample_path)\n",
    "\n",
    "            if self.downsample:\n",
    "                signal = self.downsample_waveform(signal)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # print(f'{signal.shape} = original signal shape')\n",
    "            # Clip the signal to the desired length\n",
    "            signal = self.clip(signal, sr, self.length,fixed_limit=self.fixed_limit)\n",
    "            # print(f'{signal.shape} = clipped signal shape')\n",
    "\n",
    "\n",
    "            stft = torch.stft(signal, n_fft=self.n_fft, hop_length=self.hop_length,win_length=self.n_fft, normalized=False, return_complex=True)\n",
    "            # Retransform the magnitude spectrogram back using GLA\n",
    "            _mag = torch.abs(stft) \n",
    "            _signal = self.griffin_lim(_mag)\n",
    "            _signal = _signal.reshape(1,-1) # required to get the shape right for the stft function (1,-1)\n",
    "\n",
    "\n",
    "            # INSERT FUNCTION TO GET THE LENGTH OF THE SIGNAL EQUAL TO THE ORIGINAL SIGNAL\n",
    "\n",
    "            self._update_signal_length(_signal.shape[-1])\n",
    "            _signal = self._resize_signal_length(_signal,self.signal_length)\n",
    "\n",
    "            # print(f'{_signal.shape} = resized signal shape after GLA')\n",
    "\n",
    "            _stft = torch.stft(_signal, n_fft=self.n_fft, hop_length=self.hop_length,win_length=self.n_fft, normalized=False, return_complex=True)\n",
    "\n",
    "            # print(f'{_stft.shape} = stft shape after GLA')\n",
    "\n",
    "            magnitude = self.AmplitudeToDB(torch.abs(stft)) # 25 Jul 2023 @ 12:21:38 ### CHANGED ###\n",
    "\n",
    "            print(magnitude.shape)\n",
    "            # REMOVE THE FIRST FREQUENCY BIN AND RETURN THE COMPLEX SPECTROGRAM AS TWO REAL-VALUED TENSORS\n",
    "            _stft = _stft[:,1:,...]\n",
    "            magnitude = magnitude[:,1:,...]\n",
    "            # print(f'{_stft.shape} = stft shape after removing first frequency bin')\n",
    "            # print(f'{magnitude.shape} = magnitude shape after removing first frequency bin')\n",
    "\n",
    "            real_part = _stft.real\n",
    "            imag_part = _stft.imag\n",
    "\n",
    "            # real_part, imag_part = real_part.unsqueeze(1), imag_part.unsqueeze(1)\n",
    "\n",
    "            # RETURN THE REAL AND IMAGINARY PARTS OF THE COMPLEX SPECTROGRAM AND THE MAGNITUDE SPECTROGRAM AND THE LABEL\n",
    "            return torch.cat([real_part,imag_part],dim=0), magnitude , label\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def clip(audio_signal, sr, desired_length,fixed_limit=False):\n",
    "        sig_len = audio_signal.shape[1]\n",
    "        length = int(sr * desired_length)\n",
    "\n",
    "        if fixed_limit:\n",
    "            sig = audio_signal[0][:262100]\n",
    "            return  sig.unsqueeze(0)\n",
    "\n",
    "        elif sig_len > length:\n",
    "            offset = random.randint(0, sig_len - length)\n",
    "            sig = audio_signal[:, offset:(offset+length)]\n",
    "\n",
    "            return sig\n",
    "        elif fixed_limit is None:\n",
    "            return audio_signal\n",
    "        \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def _resize_signal_length(signal, signal_length):\n",
    "        if signal.shape[-1] > signal_length:\n",
    "            signal = signal[...,:signal_length]\n",
    "            return signal\n",
    "        elif signal.shape[-1] < signal_length:\n",
    "            length_diff = signal_length - len(signal[-1])\n",
    "\n",
    "            prefix = torch.zeros((1,length_diff//2))\n",
    "            suffix = torch.zeros((1,length_diff//2))\n",
    "            signal = torch.cat([prefix,signal,suffix],dim=-1)\n",
    "\n",
    "            if len(signal[-1]) == signal_length:\n",
    "                return signal\n",
    "            else:\n",
    "                length_diff = signal_length - len(signal[-1])\n",
    "                signal = torch.cat([signal,torch.zeros((1,length_diff))],dim=-1)\n",
    "                return signal\n",
    "        else:\n",
    "            return signal\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def downsample_waveform(waveform, orig_freq=44100, new_freq=16000):\n",
    "        \"\"\"\n",
    "        Downsamples a PyTorch tensor representing a waveform.\n",
    "\n",
    "        Args:\n",
    "        waveform (Tensor): Tensor of shape (..., time) representing the waveform to be resampled.\n",
    "        orig_freq (int, optional): Original frequency of the waveform. Defaults to 44100.\n",
    "        new_freq (int, optional): Frequency to downsample to. Defaults to 16000.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Downsampled waveform.\n",
    "        \"\"\"\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=orig_freq, new_freq=new_freq)\n",
    "        return transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([1, 513, 512])\n",
      "torch.Size([4, 2, 512, 512])\n",
      "torch.Size([4, 1, 512, 512])\n",
      "('UK1', 'UK1', 'UK3', 'UK1')\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "ds = AvianNatureSounds(annotation_file_path=hp.annotation_file_path,\n",
    "                       root_dir=hp.root_dir,\n",
    "                       key=hp.key,\n",
    "                       mode=hp.mode,\n",
    "                       length=hp.length,\n",
    "                       sampling_rate=hp.sampling_rate,\n",
    "                       n_fft=hp.n_fft,\n",
    "                       hop_length=hp.hop_length,\n",
    "                       mel_spectrogram=hp.mel_spectrogram,\n",
    "                       verbose=hp.verbose,\n",
    "                       fixed_limit=True)\n",
    "\n",
    "loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "for idx, (complex, mag, label) in enumerate(loader):\n",
    "    print(complex.shape)\n",
    "    print(mag.shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 513, 52])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = torch.randn(1,26210)\n",
    "\n",
    "stft = torch.stft(sig, n_fft=1024, hop_length=512,win_length=1024, normalized=False, return_complex=True)\n",
    "\n",
    "stft.abs().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
