{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass,field\n",
    "from typing import Any, Dict, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataClassCard:\n",
    "    rank: str\n",
    "    suit: str\n",
    "    number: Union[int, str]\n",
    "\n",
    "queen_of_hearts = DataClassCard('Q', 'Hearts', [9,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen_of_hearts.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameter:\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    optimizer: str\n",
    "\n",
    "hyperparameter = Hyperparameter(0.001, 32, 10, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hyperparameter(learning_rate=0.001, batch_size=32, epochs=10, optimizer='Adam')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class _Hyperparameter():\n",
    "\n",
    "    # STFT hyperparameters\n",
    "    sr: int = 41000\n",
    "    n_fft: int = 1024\n",
    "    hop_length: int = 512\n",
    "    win_length: int = 1024\n",
    "    window: str = 'hann'\n",
    "    center: bool = True\n",
    "    return_complex: bool = True\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 4e-5\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 200\n",
    "    optimizer: str = 'Adam'\n",
    "\n",
    "    # paths\n",
    "    data_path: str = field(init=False)\n",
    "    model_path: str =  field(init=False)\n",
    "    log_path: str =  field(init=False)\n",
    "\n",
    "    def __post_init__(self, data_path, model_path, log_path):\n",
    "        self.data_path = data_path\n",
    "        self.model_path = model_path\n",
    "        self.log_path = log_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL = slice(None)\n",
    "LAST = slice(-1, None)\n",
    "HALF = slice(None, None, 2)\n",
    "NONE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3,4,5,6]\n",
    "l[HALF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1437,  1.6431, -0.2762],\n",
       "        [-1.2818,  0.2748,  0.6625],\n",
       "        [-0.9958, -0.3862, -0.2489]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "t = torch.randn(4,4,3,3)\n",
    "\n",
    "t[1,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc = nn.Linear(input_size, output_size*2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x[:,:self.output_size] * self.sigmoid(x[:,self.output_size:])\n",
    "    \n",
    "\n",
    "class GatedConvolutionalUnit:\n",
    "    def __init__(self, input_size, output_size, kernel_size, stride, padding):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.conv = nn.Conv2d(input_size, output_size*2, kernel_size, stride, padding,bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x[:,:self.output_size] * self.sigmoid(x[:,self.output_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5195,  0.3527, -0.2257,  0.3120,  0.2144],\n",
       "        [ 0.0636, -0.3561,  0.5108,  0.1135, -0.6693],\n",
       "        [ 0.0108,  0.1771,  0.1743, -0.0271, -0.0987],\n",
       "        [ 0.5395, -0.8714,  0.3512, -0.2565, -0.2042]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gatedLU = GatedLinearUnit(10, 5)\n",
    "gatedLU.forward(torch.randn(4,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (CPUComplexFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m convLU \u001b[39m=\u001b[39m GatedConvolutionalUnit(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m32\u001b[39m,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcomplex64)\n\u001b[0;32m----> 5\u001b[0m convLU\u001b[39m.\u001b[39;49mforward(x)\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mGatedConvolutionalUnit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m x[:,:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(x[:,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size:])\n",
      "File \u001b[0;32m~/Desktop/ML/SignalReconstructionML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/ML/SignalReconstructionML/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Desktop/ML/SignalReconstructionML/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (CPUComplexFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "convLU = GatedConvolutionalUnit(3, 64, 3, 1, 1)\n",
    "\n",
    "x = torch.randn(4,3,32,32,dtype=torch.complex64)\n",
    "\n",
    "convLU.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original complex array:\n",
      " [1.+2.j 2.+3.j 3.+4.j]\n",
      "\n",
      "Complex array as float32:\n",
      " [[1. 2.]\n",
      " [2. 3.]\n",
      " [3. 4.]]\n",
      "(3, 2)\n",
      "\n",
      "Real part: [1. 2. 3.]\n",
      "Imaginary part: [2. 3. 4.]\n",
      "\n",
      "Restored complex array:\n",
      " [1.+2.j 2.+3.j 3.+4.j]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a complex-valued array\n",
    "complex_array = np.array([1+2j, 2+3j, 3+4j], dtype=np.complex64)\n",
    "print(f\"Original complex array:\\n {complex_array}\")\n",
    "\n",
    "# Convert to float32 and reshape\n",
    "complex_array_float32 = complex_array.view(dtype=np.float32).reshape((*complex_array.shape, 2))\n",
    "print(f\"\\nComplex array as float32:\\n {complex_array_float32}\")\n",
    "\n",
    "print(complex_array_float32.shape)\n",
    "\n",
    "# The first channel is the real part, the second channel is the imaginary part\n",
    "real_part = complex_array_float32[..., 0]\n",
    "imaginary_part = complex_array_float32[..., 1]\n",
    "print(f\"\\nReal part: {real_part}\\nImaginary part: {imaginary_part}\")\n",
    "\n",
    "# Recombine the real and imaginary parts to restore the complex array\n",
    "restored_complex_array = complex_array_float32.view(dtype=np.complex64).reshape(complex_array.shape)\n",
    "print(f\"\\nRestored complex array:\\n {restored_complex_array}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32, 64])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_tensor = torch.randn(4,1,32,32,dtype=torch.complex64)\n",
    "# torch.randn(4,3,32,32,dtype=torch.complex64).view(dtype=torch.float32).shape\n",
    "\n",
    "\n",
    "\n",
    "complex_tensor.view(dtype=torch.float32).shape\n",
    "\n",
    "\n",
    "# real_part = complex_tensor[..., 0]\n",
    "# imaginary_part = complex_tensor[..., 1]\n",
    "\n",
    "# print(real_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6, 64, 64])\n",
      "torch.Size([8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your tensor `complex_tensor` is of shape (B, C, H, W) and dtype=torch.complex64\n",
    "\n",
    "complex_tensor = torch.randn((8, 3, 64, 64), dtype=torch.complex64)  # for example\n",
    "\n",
    "# Split the complex tensor into real and imaginary parts\n",
    "real_part = complex_tensor.real\n",
    "imag_part = complex_tensor.imag\n",
    "\n",
    "# Stack the real and imaginary parts along the channel dimension\n",
    "stacked_tensor = torch.cat((real_part, imag_part), dim=1)\n",
    "\n",
    "print(stacked_tensor.shape)  # Will output: torch.Size([8, 6, 64, 64])\n",
    "\n",
    "real_part = stacked_tensor[:, :3, ...]\n",
    "imag_part = stacked_tensor[:, 3:, ...]\n",
    "\n",
    "\n",
    "# Recombine the real and imaginary parts to restore the complex tensor\n",
    "restored_complex_tensor = real_part + 1j * imag_part\n",
    "\n",
    "print(restored_complex_tensor.shape)  # Will output: torch.Size([8, 3, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 512, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_loader import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if False:\n",
    "\n",
    "    path = '../data/AudioMNIST_Indicies/dummy_labels.csv'\n",
    "    root_dir = '../data/AudioMNIST/'\n",
    "\n",
    "else:\n",
    "    path = '../data/AvianID_AcousticIndices/UK_AI.csv'\n",
    "    root_dir='../data/UK_BIRD/'\n",
    "\n",
    "\n",
    "ds = AvianNatureSounds(annotation_file_path=path,\n",
    "                       root_dir=root_dir,\n",
    "                       key='habitat',\n",
    "                       mode='stft',\n",
    "                       length=5,\n",
    "                       sampling_rate=44100,\n",
    "                       n_fft=1024,\n",
    "                       hop_length=512,\n",
    "                       mel_spectrogram=None,\n",
    "                       verbose=False,\n",
    "                       fixed_limit=True)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=ds, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
