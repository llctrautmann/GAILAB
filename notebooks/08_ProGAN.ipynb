{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key ideas in ProGAN\n",
    "\n",
    "# - Progressive growing of the resolution\n",
    "# - Minibatch standardization\n",
    "# - Pixel Norm\n",
    "# - Equalized learning rate\n",
    "\n",
    "# Architecture\n",
    "\n",
    "# - both the descriminator and the generator are mirror images of each other\n",
    "# - much faster traiing time\n",
    "# - use minibatch standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from math import log2, sqrt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,gain=2):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels + kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0],1,1)\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,use_pixel_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels,out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels,out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pixel_norm = use_pixel_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pixel_norm else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pixel_norm else x\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim,in_channels,img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), # 1 x 1 -> 4 x 4\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1,padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            # factors[i] = factors[i] + 1\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1,padding=0))\n",
    "            \n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, z, alpha, steps):\n",
    "        out = self.initial(z)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out) # <-- potential bug note: look here if steps is correct\n",
    "\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_channels,img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1,0,-1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c,use_pixel_norm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "\n",
    "        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1,padding=0) # potential bug note: look here if steps is correct\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Block for 4 x 4 resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels+1, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, stride=1 , padding=0)\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "    \n",
    "    def minibatch_std(self, x):\n",
    "        batch_stats = torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3])\n",
    "        return torch.cat([x,batch_stats],dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0],-1)\n",
    "            \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1,len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Forward Pass of the model\n",
    "\n",
    "testing = True\n",
    "\n",
    "if testing:\n",
    "    Z_DIM = 50\n",
    "    IN_CHANNELS = 256\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS,img_channels=3)\n",
    "    critic = Discriminator(IN_CHANNELS, img_channels=3)\n",
    "\n",
    "    for img_size in [4,8,16,32,64,128,256]:\n",
    "        num_steps = int(log2(img_size / 4))\n",
    "        x = torch.randn(1, Z_DIM, 1, 1)\n",
    "        z = gen(x, 0.5, steps=num_steps)\n",
    "\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        print(f'Generator Roger Roger at image size: {img_size}')\n",
    "        out = critic(z,alpha=0.5, steps=num_steps)\n",
    "\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f'Discriminator Roger Roger at image size: {img_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop Hyperparameters\n",
    "START_TRAIN_IMG_SIZE = 4\n",
    "DATASET = 'celebhq'\n",
    "CHECKPOINT_GEN = 'generator.pth'\n",
    "CHECKPOINT_DIS = 'discriminator.pth'\n",
    "\n",
    "DEVICE = device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "SAVE_MODEL = False\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "Z_DIM = 512  # should be 512 in original paper\n",
    "IN_CHANNELS = 256  # should be 512 in original paper\n",
    "CRITIC_ITERATIONS = 1\n",
    "BATCH_SIZE = [64,64,64,32,32,32,32,16,8]\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(log2(IMAGE_SIZE)/ 4 ) + 1\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [20] * len(BATCH_SIZE)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "\n",
    "writer_fake = SummaryWriter(f'../data/runs/ProGAN/fake')\n",
    "writer_real = SummaryWriter(f'../data/runs/ProGAN/real')\n",
    "\n",
    "def plot_to_tensorboard(writer, loss_critic, loss_gen,real,fake, tb_step):\n",
    "    writer.add_scalar('loss_critic', loss_critic, tb_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_grid_fake = torchvision.utils.make_grid(real[:4], normalize=True)\n",
    "        img_grid_real = torchvision.utils.make_grid(fake[:4], normalize=True)\n",
    "\n",
    "        writer.add_image('img_grid_fake', img_grid_fake, tb_step)\n",
    "        writer.add_image('img_grid_real', img_grid_real, tb_step)\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "    \"\"\"\n",
    "    Tried using truncation trick here but not sure it actually helped anything, you can\n",
    "    remove it if you like and just sample from torch.randn\n",
    "    \"\"\"\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
    "    gen.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5 for _ in range(CHANNELS)], [0.5 for _ in range(CHANNELS)]),\n",
    "        ])\n",
    "\n",
    "    batch_size = BATCH_SIZE[int(log2(image_size / 4))]\n",
    "\n",
    "    print(batch_size,)\n",
    "    dataset = datasets.ImageFolder(root='../data/celebhq_img/',transform=transform)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=NUM_WORKERS,pin_memory=True)\n",
    "\n",
    "    return loader, dataset\n",
    "\n",
    "\n",
    "def train(critic,\n",
    "            gen,\n",
    "            loader,\n",
    "            dataset,\n",
    "            step,\n",
    "            alpha,\n",
    "            opt_critic,\n",
    "            opt_gen,\n",
    "            tensorboard_step,\n",
    "            writer\n",
    "            ):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1, device=device)\n",
    "\n",
    "        fake = gen(noise, alpha, step)\n",
    "        critic_real = critic(real,alpha,step)\n",
    "        critic_fake = critic(fake.detach(),alpha,step)\n",
    "\n",
    "        gp = gradient_penalty(critic, real, fake,alpha, step,device=DEVICE)\n",
    "\n",
    "        loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp + (0.001 * torch.mean(critic_real.pow(2)))\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "\n",
    "        # Train Generator max E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake, alpha, step)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        alpha  += cur_batch_size / (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset) \n",
    "        alpha = min(1, alpha)\n",
    "\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "\n",
    "            plot_to_tensorboard(writer,\n",
    "                                loss_critic.item(),\n",
    "                                loss_gen.item(),\n",
    "                                real.detach(),\n",
    "                                fixed_fakes.detach(),\n",
    "                                tb_step=tensorboard_step)\n",
    "            \n",
    "            tensorboard_step += 1\n",
    "\n",
    "    return tensorboard_step, alpha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    gen = Generator(z_dim=Z_DIM, in_channels=IN_CHANNELS, img_channels=CHANNELS).to(DEVICE)\n",
    "    critic = Discriminator(in_channels=IN_CHANNELS, img_channels=CHANNELS).to(DEVICE)\n",
    "\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE,betas=(0.0,0.99))\n",
    "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE,betas=(0.0,0.99))\n",
    "\n",
    "    writer = SummaryWriter(f'../data/runs/ProGAN/log')\n",
    "\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    step = int(log2(START_TRAIN_IMG_SIZE / 4))\n",
    "\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5 \n",
    "        loader, dataset = get_loader(4 * 2 ** step)\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer\n",
    "            )\n",
    "\n",
    "            if SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_DIS)\n",
    "\n",
    "        step += 1  # progress to the next img size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
