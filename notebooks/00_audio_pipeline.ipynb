{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/Users/luca/Desktop/ML/SignalReconstructionML/data/AudioMNIST'\n",
    "OUTPUT_FOLDER = '/Users/luca/Desktop/ML/SignalReconstructionML/data/ProcessedAudioMNIST'\n",
    "SAMPLE_RATE = 22050  # Adjust according to your audio files\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of audio files in the data folder\n",
    "audio_files = [file for file in os.listdir(DATA_FOLDER) if file.endswith('.wav')]\n",
    "\n",
    "tensors = []\n",
    "\n",
    "for i in range(0, 200):\n",
    "    file = audio_files[i]\n",
    "    file, _ = librosa.load(f'/Users/luca/Desktop/ML/AutoencoderML/data/AudioMNIST/{file}')\n",
    "\n",
    "    stft = librosa.stft(file,n_fft=FRAME_SIZE,hop_length=HOP_SIZE, window='hann')\n",
    "\n",
    "    if stft.shape[1] < 400:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        magnitude = librosa.power_to_db(np.abs(stft[:,0:400]))\n",
    "        phase = np.angle(stft[:,0:400])\n",
    "\n",
    "        mag_tensor = torch.tensor(magnitude).unsqueeze(0)\n",
    "        phase_tensor = torch.tensor(phase).unsqueeze(0)\n",
    "        \n",
    "        t = torch.cat((mag_tensor, phase_tensor), dim=0)\n",
    "\n",
    "        tensors.append(t.unsqueeze(0))\n",
    "        \n",
    "# Convert the list of tensors into a single tensor\n",
    "images_tensor = torch.cat(tensors, dim=0)\n",
    "\n",
    "# Create a TensorDataset from the image tensor\n",
    "dataset = TensorDataset(images_tensor)\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "\n",
    "# Create a data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "with open('../data/ProcessedAudioMNIST/DataLoader.pkl', 'wb') as file:\n",
    "    pickle.dump(data_loader, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
